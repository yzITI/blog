---
title: 一个集群有什么组件
date: 2020-05-08 00:00:00
categories:
- 科普
tags:
- 集群
toc: true
---

> 如果对集群感兴趣的话，不妨来看看一个集群都有哪些组件。
> 本文基本由Nick大佬讲述，我记录整理成文。

<!-- more -->

## 从docker开始

首先我们还是先从为什么要用kubernetes这类集群管理器开始。

在很久很久以前大家如果想在同一台服务器上稳定运行多个应用只能使用虚拟机，而虚拟机大家都知道是非常耗资源的，毕竟一整个系统内核都在里面跑。于是有了一些更加先进的虚拟化技术，例如OpenVZ就是一个代表。OpenVZ通过共享一部分宿主机的内核的形式使虚拟机的开销最小化，虽然这样限制了虚拟机和宿主机的内核都只能是Linux，但这种虚拟化技术比起直接运行一整个内核开销要低得多。准确的说，OpenVZ划分出的虚拟机就已经是容器了。

而其它一些unix系统也有类似的自己的虚拟化技术。例如solaris有区域的概念，它是solaris系统内核的一部分，同样是通过共享内核的方法降低开销，所以这也被称作solaris容器。容器的优势是每个容器都可以使用宿主机所有的资源，而不会被限制在某一个CPU核心上。因为这种容器化的虚拟机显示出很大优势，inux内核在2007年加入了完全虚拟化的KVM技术后也终于在2008年加入了Linux Container，而docker就是一个Linux Container的包装。

Linux Container就是来自Solaris区域的一个功能，它并不是非常的用户友好，而且与虚拟机类似可以同时运行多个进程，并且容器内部有状态，和微服务的理念不太一致。更重要的是Linux Container 不具备可移植性。为了弥补这些缺陷，Docker就给它加了一层包装，让每个容器只能运行一个应用，并且提供了多样化的镜像用于快速部署。Docker用来管理这些容器的服务就叫做containerd Docker 推出后很快就因为它简单易用广受好评，然而企业在实际运用中遇到了另一个问题。Docker的目标是微服务，微服务就要求将所有的组建拆开分到不同的Docker容器中分别管理。如果是小公司可能只需要十到二十个容器，运维人员还是可以手动管理的。但大公司通常需要管理数个不同数据中心的上万个容器，这些容器有不同的可扩展性，不同的可用性要求，使用不同的资源，这样根本不可能让运维人员直接管理这些容器。因此人们就需要一个管理容器的服务，被称作orchestration。

在kubernetes和docker出现前已经有了一个管理集群的工具叫做mesos，是Apache基金会下的一个项目，它配备一个container orchestration  叫marathon，这个服务框架并不是专门为了docker开发的，而是作为一个万能的集群管理框架使用。虽然mesos在docker出来之后已经有了一个管理docker的组建，但mesos是极端的难配置。因为它实在是太难配置了所以后来有一个公司Mesosphere基于它开发了DC/OS，配备了一个简单易用的网页UI和cli，但这个等会儿再说。

说回docker，在dotCloud公司开源docker之后，很多公司例如google就立刻开始使用docker部署应用。一开始Google将docker融入自家的集群管理系统Borg，但很快Google的一些工程师就决定给docker造一个新的轮子，这个轮子起源于Borg，但是将它简化了很多，专注于调度容器化应用，而这个轮子就是Kubernetes。

### Kubernetes
Kubernetes是一个主从架构的集群管理框架，在主服务器上运行了三个核心服务：kube-scheduler，kube-api-server和etcd，另外还有控制kubernetes正确运行的服务kube-controller-manager，如果建在云上还可以使用cloud-controller-manager动态管理云服务器。kube-scheduler就是kubernetes的主调度器，它根据标签调度pod实现负载均衡，etcd是一个存储服务，提供高可用的存储后端存储kubernetes的集群数据，而kube-api-server是一个中枢，它负责处理所有和scheduler, etcd 和controller 的通信与所有从服务器的通信。从服务器上运行了两个组件，kubelet是管理pod的组件，它负责确保pod的正常运行，而kube-proxy是控制网络通信的组件，所有从服务器的通信都通过kube-proxy完成。kube-proxy可以设置一系列规则来控制pod的流量。

Kubernetes在管理容器方面表现很突出，但是也有几个限制，例如容器应用的目标是无状态应用，但很多应用是有状态的，例如消息队列肯定是要存储状态，因此在kubernetes山搭建有状态的集群比较困难。另外kubernetes一个主节点最大只能支持5000个子节点，应用的拓展能力有限。并且很多应用在容器化环境下还没有经过充分测试，可能会导致服务不稳定。因此如果要搭建更加庞大的分布式系统的话将Kubernetes融入DC/OS是一个更加明智的选择。

![265AC2B06869A5A8F5435EF9C9F6149D.png](https://i.loli.net/2020/05/08/HVRIDvCXn6QKTSL.jpg)


在DC/OS的最上层是用户界面，有GUI，CLI和其他一些应用程序接口。同样DC/OS也有主节点和从节点。主节点（Master Nodes ） 包含集群管理的一些组件，最主要的是mesos master 和zookeeper ，另外DC/OS包含一个默认的容器管理系统：负责管理长时间运行的应用的Marathon和负责管理短时间运行的任务的Metronome。另外还包括了和Kubernetes类似的网络框架和验证系统。而在从节点（Agent Nodes）上实际运行用户应用，通过Mesos Agent和主节点通信。Mesos可以同时管理有状态和无状态的应用，因此它内置了一个Docker Engine，而Universal Container Runtime 则可以使用任何容器化方案或者让Mesos为用户应用包一层容器。

![3F91D0EF4CE83792F9D4F822A0E7DC99.png](https://i.loli.net/2020/05/08/LVmCdgfkeT2yilH.jpg)

下面看一看DC/OS是怎么调度应用的。可以看到DC/OS有两层调度，第一层是一个scheduler，默认来说就是Marathon，而第二层就是Mesos Master 。在用户刚刚启动服务的时候

(1)调度器还没有任务需要处理，这时Mesos主节点会从从节点收集资源信息并向调度器发出资源邀约
(2)而调度器会拒绝这个邀约

(3)直到用户启动了一个应用。当Mesos再一次发出资源邀约时

(5)调度器会接受邀约

(6)并要求主节点分配资源。主节点会向一个活跃的空闲子节点发出启动命令

(7)而子节点会向对应的运行时发出启动命令

(8)运行时启动完毕后就会返回状态更新给子节点

(9)而子节点会将这个消息回传给主节点
(10)主节点再将消息传回调度器
(11)最后调度器将这个消息传给用户

而这个结构就决定了DC/OS是高度可扩展的。例如我们想让Kubernetes管理docker容器，而让Marathon管理剩下的容器，只需要将docker调度器向Mesos主节点注册，然后让mesos主节点在mesos子节点上安装etcd，api-server 和kubelet，这样kubernetes调度器就能像marathon一样接受mesos主节点的资源要约，并要求mesos主节点在mesos子节点上部署kubernetes运行时，让kubernetes和mesos完全融合在一起。

其他类似的分布式框架都可以用同样的方式部署，例如kafka会将Kafka调度器注册到mesos主节点，当需要Kafka的请求发来时，Kafka调度器就可以直接向mesos主节点转发请求到Kafka节点上，不仅能让所有框架以近乎原生的方式同时运行还能将所有的节点纳入DC/OS的管理下。

## Zookeeper
这也是十分复杂的东西，是很多集群高可用的基础。
首先，在讲zookeeper时就要讲zab算法，而zab算法又是multi-paxos的变形，multi-paxos又是paxos算法的工程实现，而paxos算法是谁提出来的呢？就是牛逼的兰伯特。
当然paxos太过理论，直接讲一讲ZAB就好了，ZAB是Zookeeper Atomic Boardcast，缩写，是基于multi-paxos算法完善的一个协议，协议中定义了三个不同的角色：领导者，追随者和观察者。领导者是整个集群唯一能够写入事务的服务器，而追随者会和领导者保持同步，负责处理客户端的读请求，观察者同样也会和追随者同步缓解追随者的压力，观察者和追随者的区别在于追随者拥有选举权，被选举权和表决权，而观察者没有。下面具体讲一讲zookeeper是怎么工作的。

zookeeper是一个分布式锁系统，负责让集群保持同步的同时提供高可用的服务。但是基于CAP定理一致性（Consistency）高可用（Availability）和分区容错性(Partition tolerance)不可兼得，因此zookeeper不是基于传统的ACID模型，也就是强一致的原子性-一致性-隔离性-持久性的数据库模型，而是基于更弱的BASE模型，BA是Basically Availability，也就是集群要保持基本可用的状态，即使数据不一致也不会使集群完全瘫痪，S是Soft state，也就是数据可能有一个中间状态，在这个状态集群可能不同步，E是Eventually consistent，也就是最终一致性，只要在事务结束时获得一致性就可以了，在中间过程中即使没有同步也没有关系。

ZAB协议是这个过程的关键。它遵守一个复杂的过程使领导者，追随者和观察者以最快速度同步。首先要介绍的是ZAB的编号系统zxid，它是一个64位长整型数字，前32位存储了纪元，每一次领导者选举都会使纪元+1，后32位是一个简单的32位递增计数变量，在更新纪元后会清空，zookeeper利用这个变量确保事务同步。另外每个节点都有一个myid编号，是这个节点在集群中的唯一编号。在一个zookeeper集群启动后，会开始一轮领导者选举。选举过程是每个服务器会产生一个选票，选票包含两个数值，第一个是自己的myid，第二个是自己存储的最新事务编号zxid。每个节点会先投自己一票，在收到其他节点的选票时如果另一个节点的zxid比自己的大，就会重新发出新的选票投zxid较大的节点。如果zxid大小一样但是myid更大，那么会选择myid更大的节点重新投票。当节点统计出某一个节点已经收到超过半数的选票时，就会认为已经选出了新的领导者，然后改变自己的状态。领导者的状态是LEADING，跟随着的状态是FOLLOWING，观察者的状态是OBSERVING。当选举过程没有结束时，节点状态是LOOKING。当一个集群第一次启动时，集群中还没有事务，因此永远是myid最大的节点当选为领导者。领导者在选出后还不能立刻成为新的领导者，它需要进行一次节点发现的过程。具体来说，确定领导者后追随者会讲自己最新的纪元值发送给领导者，领导者收到半数追随者的纪元值后会发送一个更新的纪元值给这些发来纪元值的节点。如果追随者的纪元值不统一，领导者会选择其中最大的一个+1后发送给追随者。追随者收到新的纪元值后会将这个纪元值存储并发出ACK消息和一个自己的历史事务记录。领导者会记录所有回应的追随者并选择所有历史记录里zxid最大的一个作为初始消息集合同步给其他追随者。同步阶段领导者会将这个初始消息集合和当前纪元值发送给所有在线的追随者。追随者会确认领导者的纪元和自己的纪元相同，如果不相同说明自己落后太远，需要等待下一轮发现过程再同步。而如果纪元相同，追随者就会向领导者确认自己已经收到了历史记录。当领导者收到过半的确认时，就会发出提交命令，要求所有追随者提交所有没有处理的事务，至此同步阶段就结束了。同步结束后，zookeeper集群就可以开始处理客户端的请求了。对于客户端的读请求追随者可以直接回应，而当追随者收到写请求时会将写请求转发给领导者，领导者就会生成一个新的事务提案并附上一个zxid，然后向所有追随者发送提案。追随者根据接受顺序处理提案并投票，当领导者收到半数的投票后，就会发出提交命令要求所有追随者提交这个事务，这个事务写入的内容就会开始向外提供服务。

这种先提案后提交的协议叫做两阶段提交，选举过程使用的算法是快速选举算法。在领导者崩溃后选举是集群进入了恢复模式，恢复模式有三个原则，第一个是原来的领导者不能收到半数追随者的心跳包时就会主动放弃领导者的职位，使集群进入选举状态。第二个原则是已经被提交的事务不会丢失，因此领导者选举时会选择zxid最大的一个。第三个原则是没有被提交的事务要被丢弃，因此领导者需要比较所有的追随者的历史记录，丢弃没有半数节点收到的提案，并选择一个zxid最大的历史记录集合同步。


## 高可用
最后讲讲高可用。可用性是衡量一个集群的重要标准。集群的高可用目标一般至少是4个9，也就是99.99%，说明一年内有99.99% 的时间服务都在线。而达成高可用的核心是冗余，用多个服务器提供同一个服务，而同时还要有故障转移，灾难恢复，可用性降级的能力。而冗余就带来另一个问题，也就是多个服务怎么才能实时同步，也就是集群的一致性，同时还要能保证性能。可用性，一致性和性能是分布式服务的衡量标准。为了能同步节点，集群势必要使用主从结构，由主节点决定如何更新消息，从节点与主节点同步消息。

刚刚好像用主从节点代表了kubernetes和mesos的主节点和工作节点的关系，那个有点不对，主从节点应该是从节点始终与主节点同步。kubernetes和mesos的主节点向工作节点发送任务不是这个架构。

既然有主从结构，那么就必须要决定谁才是主节点，而且还要顾及到主节点如果宕机了那么新的主节点怎么选出。为了解决这个问题人们提出了分布式选举算法。简单来说，例如有7个节点，原本的1号节点是主节点，突然2号和4号节点检测到1号节点失去响应，它们就会向其余节点发送选举通知，剩余的节点采取先到先得原则投票。当2号或4号节点在一轮投票中收到半数以上的票时，就会自动成为新的主节点，并将自己成为主节点的通知发送给其他节点，剩余节点会自动和新的主节点同步。

而如果集群中出现了网络分区的问题，例如123号节点和4567号节点间的交换机出现故障，那么4567同时和主节点断开了联系，并选出一个新的主节点，而1号节点依然认为自己是主节点，就出现了两个主节点。此时集群的一致性就大大降低了。而多数派机制就是为了解决这个问题而生的。如果选举过程中没有达到集群中所有节点的一半以上，那么选举就是无效的，因此123号节点在发现无法连接到4567号节点时就不会产生新的主节点，而所有流量都会传递到多数节点侧的主节点上

但是节点确认自己是否成为少数派需要一定时间，在这段时间里集群依然在提供服务，而如果一部分请求在新的主节点上修改了数据而另一部分则在老的主节点上读取数据依然会导致集群无法保证一致性。此时就需要另一个租约机制。租约机制引入了一个新的主节点概念：区域主节点。具体来说，整个网络中任意时刻总会有且仅有一个节点获得租约，获得租约的节点就是这个网络的区域主节点。所有的请求都会先发给区域主节点，但区域主节点可能和选举的主节点不一致，此时区域主节点会将请求转发给当前的主节点。当发生网络分区时，如果区域主节点在多数派一侧，由于客户端请求在租约过期前都会发给区域主节点，因此不论网络环境怎样区域主节点都会收到请求。如果区域主节点无法连接到选举的主节点，那么区域主节点会向其他所有在线的节点发出问询，如果收到了一半以上的确认那么区域主节点就会自动成为新的主节点然后接受请求，根据算法不同可能会在多数派一侧进行一轮选举。如果没有收到一半以上的确认那说明区域主节点在少数派一侧，那么此时集群就处于不可用状态，直到区域主节点租约过期，而租约产生时需要确认获得租约的节点能收到一半以上节点的确认，因此租约和新的选举都会在多数派侧产生。但租约机制不能解决拜占庭问题。而如果区域主节点和选举主节点同时落在了少数派一侧，那么此时少数派的主节点都因为票数不够无法工作，直到租约结束多数派重新选举后集群才会恢复。

但是这个机制不能解决拜占庭将军问题，具体这个问题是啥已经可以写一篇论文了https://dl.acm.org/doi/pdf/10.1145/3335772.3335936 图灵奖得主兰伯特的著作

分布式选举算法可以看看paxos和raft算法，paxos就是兰伯特提出的解决分布一致性的算法。raft是新的，用的比较广泛的，比较容易理解的算法。而paxos问题的原论文是一个数学证明，所以比较生涩难懂。

这儿有个讲raft的动画：http://thesecretlivesofdata.com/raft/ 。

### 拜占庭将军问题

就是有9个拜占庭帝国的将军，他们身在帝国各地打仗，现在他们需要作出一个统一的，决定进攻还是后退的决策，因此他们派出传令兵传话，少数服从多数。但问题是在这9个将军里有1个是叛军，这个叛军给4个将军传后退的决定，另外4个传进攻的决定，这样整个军队的一致性就被破坏，或者传令兵可能会是叛徒，故意调换信件。
在分布式系统中，将军就是节点，而传令兵就是通讯系统。由于分布式系统的不可靠性，在通信过程或者产生决策的过程中可能系统会因为接收到错误的信息产生错误的决策，拜占庭将军问题就是描述这个问题。
不过这种问题在机房环境很少遇见，一般考虑这个问题的分布式系统是飞机啊，火箭啊，核电站啊这种地方。

